# LLM as Judge Feature Specifications

This document outlines the inputs and outputs for the core components of the LLM-based evaluation judging feature.

## 1. Core Judging Function

This function handles the evaluation of a single translation item.

*   **Location:** `app.services.judge_service.evaluate_translation` (Proposed)
*   **Purpose:** Evaluate a single translation using a specified Judge LLM and prompt.
*   **Inputs:**
    *   `source_text: str`: The original source text segment.
    *   `model_output: str`: The machine translation output generated by the prompt being tested.
    *   `judge_model_id: str`: Identifier for the LLM to be used as the judge (e.g., `"claude-3-opus-20240229"`, `"gpt-4-turbo"`).
    *   `criteria_prompt_template: str`: The prompt template instructing the Judge LLM. Must include placeholders (e.g., `{source}`, `{translation}`, `{reference}`) and request a structured output (ideally JSON).
    *   **`reference_materials: Optional[Dict[str, str]] = None`**: A dictionary holding various reference types. Keys might include `"human_reference"`, `"tm_matches"`, `"termbase_entries"`, `"style_guide_rules"`, etc. The prompt template should reference these keys, and the formatting logic should handle their optional presence.
*   **Outputs:** (Dictionary or Pydantic Model, e.g., `JudgeResult`)
    *   `status: str`: Outcome indicator (`'success'` or `'error'`).
    *   `score: Optional[float]`: Numerical score from the judge response (if successful and requested).
    *   `rationale: Optional[str]`: Textual rationale from the judge response (if successful and requested).
    *   `error_message: Optional[str]`: Details on failure if `status` is `'error'`.
    *   `raw_judge_output: Optional[str]`: The complete, raw text response from the Judge LLM.
    *   `parsed_output: Optional[dict | Any]`: The parsed structured output (e.g., dictionary from JSON) if successful.

## 2. API Endpoint

This endpoint triggers the judging process for a full evaluation run.

*   **Path:** `POST /api/v1/evaluations/{evaluation_id}/judge`
*   **Purpose:** Schedule background tasks to perform LLM judging on all results within a specific evaluation run.
*   **Inputs:**
    *   **Path Parameter:**
        *   `evaluation_id: PyObjectId`: The unique ID of the parent `Evaluation` document.
    *   **Body (Optional - Future):**
        *   `judge_model_id: Optional[str]`: Override default judge model.
        *   `criteria_prompt_template: Optional[str]`: Override default judge prompt.
    *   **Headers:**
        *   `Authorization: Bearer <JWT_token>` (Handled by dependency).
*   **Outputs (HTTP Responses):**
    *   **`202 Accepted`:** Request accepted, background task(s) scheduled. Empty or minimal confirmation body.
    *   **`401 Unauthorized`:** User not authenticated.
    *   **`403 Forbidden`:** Authenticated user does not own the `evaluation_id`.
    *   **`404 Not Found`:** `evaluation_id` does not exist.
    *   **`409 Conflict`:** (Optional) Judging already in progress or completed for this evaluation.
    *   **`500 Internal Server Error`:** Unexpected backend error (e.g., task scheduling failed). 