from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, List
from datetime import datetime
from bson import ObjectId

from .common import PyObjectId

from .prompt import PromptSection # Assuming PyObjectId is now defined in prompt.py (adjust if moved)

# --- Evaluation Result Models ---

class EvaluationResultBase(BaseModel):
    """Base attributes for a single row in an evaluation result."""
    evaluation_id: PyObjectId = Field(..., description="The ID of the parent Evaluation session.")
    prompt_id: PyObjectId = Field(..., description="The ID of the specific Prompt version used for this result.")
    source_text: str = Field(..., description="The original source text provided.")
    model_output: Optional[str] = Field(None, description="The output generated by the AI model.")
    reference_text: Optional[str] = Field(None, description="The reference translation (if provided).")
    score: Optional[int] = Field(None, ge=1, le=5, description="Manual score assigned (e.g., 1-5).")
    comment: Optional[str] = Field(None, max_length=1000, description="User comments or feedback.")

class EvaluationResultCreate(EvaluationResultBase):
    """Properties needed to create an evaluation result (internal use)."""
    # Initially created without score/comment
    score: Optional[int] = None
    comment: Optional[str] = None

class EvaluationResultUpdate(BaseModel):
    """Properties allowed for updating a result (score and comment)."""
    score: Optional[int] = Field(None, ge=1, le=5)
    comment: Optional[str] = Field(None, max_length=1000)

    # Ensure at least one field is provided for update
    model_config = ConfigDict(
        validate_assignment=True,
        extra='forbid'
    )

    # Optional: Add a root validator if needed to ensure at least one field is not None
    # @root_validator(pre=True)
    # def check_at_least_one_value(cls, values):
    #     if not any(values.values()):
    #         raise ValueError("At least one field (score or comment) must be provided for update")
    #     return values

class EvaluationResultInDB(EvaluationResultBase):
    """Model representing an evaluation result stored in MongoDB."""
    id: PyObjectId = Field(
        default_factory=PyObjectId,
        validation_alias="_id"
    )
    created_at: datetime = Field(default_factory=datetime.utcnow)

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        json_encoders={ObjectId: str, PyObjectId: str},
         json_schema_extra={"example": {
            "_id": "6eb7cf5a86d9755df3a6c5a1",
            "evaluation_id": "5eb7cf5a86d9755df3a6c593",
            "source_text": "Hello world",
            "model_output": "Bonjour le monde",
            "reference_text": "Salut le monde",
            "score": 4,
            "comment": "Slightly too formal.",
            "created_at": "2023-10-27T11:00:00Z"
        }}
    )

class EvaluationResult(EvaluationResultInDB):
    """Properties to return to the client via API for an evaluation result."""
    pass

# --- Evaluation Session Models ---

class EvaluationRequestData(BaseModel):
    """Structure for individual items in the test set data."""
    source_text: str
    reference_text: Optional[str] = None
    # Add other optional fields like text_id, extra_info if needed from PRD FR-EV-03
    # text_id: Optional[str] = None
    # extra_info: Optional[dict] = None

class EvaluationCreateRequest(BaseModel):
    """Request body for initiating a new evaluation session."""
    prompt_ids: List[PyObjectId] = Field(..., min_length=1, description="List of Prompt version IDs to evaluate.")
    test_set_data: List[EvaluationRequestData] = Field(..., min_length=1, description="List of source texts and optional references.")
    test_set_name: Optional[str] = Field(None, max_length=100, description="Optional name for this test run/set.")

class EvaluationBase(BaseModel):
    """Base attributes for an Evaluation session."""
    prompt_ids: List[PyObjectId] = Field(..., description="List of Prompt version IDs evaluated in this session.")
    test_set_name: Optional[str] = Field(None, description="Name of the test set used, if provided.")
    status: str = Field(default="pending", description="Status of the evaluation (e.g., pending, running, completed, failed).")

class EvaluationInDB(EvaluationBase):
    """Model representing an evaluation session stored in MongoDB."""
    id: PyObjectId = Field(
        default_factory=PyObjectId,
        validation_alias="_id"
    )
    created_at: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None
    test_set_data: List[EvaluationRequestData]
    total_prompt_tasks: Optional[int] = Field(None, description="Total number of prompt evaluation tasks expected.")
    completed_prompt_tasks: Optional[int] = Field(default=0, description="Number of prompt evaluation tasks completed.")
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        json_encoders={ObjectId: str, PyObjectId: str},
         json_schema_extra={"example": {
            "_id": "5eb7cf5a86d9755df3a6c593",
            "prompt_id": "4eb7cf5a86d9755df3a6c582",
            "test_set_name": "Marketing Batch 1",
            "status": "completed",
            "created_at": "2023-10-27T10:55:00Z",
            "completed_at": "2023-10-27T11:05:00Z",
            "test_set_data": [{"source_text": "Hello", "reference_text": "Salut"}]
        }}
    )

class Evaluation(EvaluationInDB):
    """Properties to return to the client via API for an evaluation session."""
    # Hide the potentially large test_set_data from the default response
    model_config = ConfigDict(
        # Pydantic v2 way to exclude fields from serialization
        fields={'test_set_data': {'exclude': True}}
    ) 